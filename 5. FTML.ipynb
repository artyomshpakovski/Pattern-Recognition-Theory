{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1acc842a",
   "metadata": {},
   "source": [
    "#### Использовать выборку MNIST 80% обучающая выборка 20% тестовая"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b6722fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model with hidden layer sizes: 5, 5\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 157\u001b[0m\n\u001b[1;32m    155\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining model with hidden layer sizes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhs1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhs2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 157\u001b[0m train_model(model, train_loader, optimizer, criterion, epochs\u001b[38;5;241m=\u001b[39mepochs)\n\u001b[1;32m    158\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m test_model(model, test_loader)\n\u001b[1;32m    159\u001b[0m data[hs2]\u001b[38;5;241m.\u001b[39mappend(accuracy)\n",
      "Cell \u001b[0;32mIn[56], line 110\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, criterion, epochs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    109\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 110\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    111\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:856\u001b[0m, in \u001b[0;36mOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure: Optional[Callable[[], \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    846\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs a single optimization step (parameter update).\u001b[39;00m\n\u001b[1;32m    847\u001b[0m \n\u001b[1;32m    848\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;124;03m        ``.grad`` field of the parameters.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd # type: ignore\n",
    "\n",
    "# Импорт пользовательского оптимизатора FTML\n",
    "class FTML(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr, beta1=0.6, beta2=0.999, epsilon=1e-8):\n",
    "        # Проверка на валидность входных параметров\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if beta1 < 0.0 or beta1 >= 1.0:\n",
    "            raise ValueError(\"Invalid beta1 parameter: {}\".format(beta1))\n",
    "        if beta2 < 0.0 or beta2 >= 1.0:\n",
    "            raise ValueError(\"Invalid beta2 parameter: {}\".format(beta2))\n",
    "        if epsilon < 0.0:\n",
    "            raise ValueError(\"Invalid epsilon parameter: {}\".format(epsilon))\n",
    "        # Инициализация параметров оптимизатора\n",
    "        defaults = dict(lr=lr, beta1=beta1, beta2=beta2, epsilon=epsilon)\n",
    "        super(FTML, self).__init__(params, defaults)\n",
    "\n",
    "def step(self, closure=None):\n",
    "    loss = None\n",
    "    if closure is not None:\n",
    "        loss = closure()\n",
    "\n",
    "    # Проход по всем группам параметров\n",
    "    for group in self.param_groups:\n",
    "        # Проход по всем параметрам в группе\n",
    "        for p in group['params']:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            grad = p.grad.data\n",
    "            state = self.state[p]\n",
    "\n",
    "            # Инициализация состояния, если оно пустое\n",
    "            if len(state) == 0:\n",
    "                state['t'] = 0\n",
    "                state['v_hat'] = torch.zeros_like(p.data)\n",
    "                state['z'] = torch.zeros_like(p.data)\n",
    "                state['d'] = torch.zeros_like(p.data)\n",
    "\n",
    "            state['t'] += 1\n",
    "            t = state['t']\n",
    "            beta1, beta2 = group['beta1'], group['beta2']\n",
    "            epsilon = group['epsilon']\n",
    "\n",
    "            # Обновление состояний v_hat и z\n",
    "            state['v_hat'] = beta2 * state['v_hat'] + (1 - beta2) * grad\n",
    "            state['z'] = beta1 * state['z'] + (1 - beta1) * grad\n",
    "\n",
    "            # Коррекция смещения для v_hat и z\n",
    "            v_hat_bias_corr = state['v_hat'] / (1 - beta2 ** t)\n",
    "            z_bias_corr = state['z'] / (1 - beta1 ** t)\n",
    "\n",
    "            # Вычисление направления d и обновление состояния d\n",
    "            d = -state['z'] / (torch.sqrt(v_hat_bias_corr) + epsilon)\n",
    "            state['d'] = beta1 * state['d'] + (1 - beta1) * d\n",
    "\n",
    "            # Обновление параметра p\n",
    "            p.data += group['lr'] * state['d']\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Загрузка данных и преобразование\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "mnist_data = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_size = int(0.8 * len(mnist_data))\n",
    "test_size = len(mnist_data) - train_size\n",
    "train_data, test_data = random_split(mnist_data, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Определение модели\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        # Определение слоев модели\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Прямой проход модели\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Обучение модели с алгоритмом FTML\n",
    "def train_model(model, train_loader, optimizer, criterion, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.view(inputs.shape[0], -1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Оценка модели на тестовой выборке\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.view(inputs.shape[0], -1)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy on test set: {accuracy}\")\n",
    "    return accuracy\n",
    "\n",
    "# Параметры модели и обучение\n",
    "input_size = 28 * 28\n",
    "output_size = 10\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# for hidden_size1 in [5, 10, 15, 20, 25]:\n",
    "#     for hidden_size2 in [5, 10, 15, 20, 25]:\n",
    "#         model = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "#         optimizer = FTML(model.parameters(), lr=learning_rate)  # Использование пользовательского оптимизатора FTML\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         print(f\"\\nTraining model with hidden layer sizes: {hidden_size1}, {hidden_size2}\")\n",
    "#         train_model(model, train_loader, optimizer, criterion, epochs=epochs)\n",
    "#         test_model(model, test_loader)\n",
    "\n",
    "# Фрагмент кода для заполнения данных точности моделей в таблицу\n",
    "hidden_sizes = [5, 10, 15, 20, 25]\n",
    "data = {hs2: [] for hs2 in hidden_sizes}\n",
    "index = []\n",
    "\n",
    "for hs1 in hidden_sizes:\n",
    "    index.append(hs1)\n",
    "    for hs2 in hidden_sizes:\n",
    "        model = MLP(input_size, hs1, hs2, output_size)\n",
    "        optimizer = FTML(model.parameters(), lr=learning_rate)  # Использование пользовательского оптимизатора FTML\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        print(f\"\\nTraining model with hidden layer sizes: {hs1}, {hs2}\")\n",
    "        train_model(model, train_loader, optimizer, criterion, epochs=epochs)\n",
    "        accuracy = test_model(model, test_loader)\n",
    "        data[hs2].append(accuracy)\n",
    "\n",
    "df = pd.DataFrame(data, index=index, columns=hidden_sizes)\n",
    "\n",
    "# Вывод таблицы данных\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ce0c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899b3b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee6df26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b2263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e61db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05455bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b55a0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
